#!/usr/bin/env python3
"""Generate a markdown table summarizing eval JSONs in eval_results.
Writes the file to docs/eval_results_table.md and appends rows as files are processed.
"""
import json
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
EVAL_DIR = ROOT / "eval_results"
OUT_MD = ROOT / "docs" / "eval_results_table.md"

# Columns for the table
COLUMNS = [
    "model_folder",
    "file",
    "model_name",
    "dataset",
    "attack_type",
    "poison_pct",
    "defense",
    "test_accuracy",
    "weighted_f1",
    "macro_f1",
    "pos_precision",
    "pos_recall",
    "tn",
    "fp",
    "fn",
    "tp",
    "num_test_samples",
]


def extract_metrics(d):
    # Safely extract metrics from a JSON dict
    model_name = d.get("model_name", "")
    dataset = d.get("dataset_name", "")
    attack_type = d.get("attack_type", "")
    poison_pct = d.get("poisoning_percentage", "")
    defense = d.get("defense_strategy", d.get("mitigation", ""))
    test_acc = d.get("test_accuracy")
    num_test = d.get("num_test_samples")

    class_report = d.get("classification_report", {})
    weighted_f1 = None
    macro_f1 = None
    pos_precision = None
    pos_recall = None
    if isinstance(class_report, dict):
        wa = class_report.get("weighted avg") or class_report.get("weighted_avg") or class_report.get("weighted")
        ma = class_report.get("macro avg") or class_report.get("macro_avg") or class_report.get("macro")
        if wa and isinstance(wa, dict):
            weighted_f1 = wa.get("f1-score")
        if ma and isinstance(ma, dict):
            macro_f1 = ma.get("f1-score")
        pos = class_report.get("1") or class_report.get("positive")
        if pos and isinstance(pos, dict):
            pos_precision = pos.get("precision")
            pos_recall = pos.get("recall")

    cm = d.get("confusion_matrix", {})
    tn = cm.get("true_negative")
    fp = cm.get("false_positive")
    fn = cm.get("false_negative")
    tp = cm.get("true_positive")

    return {
        "model_name": model_name,
        "dataset": dataset,
        "attack_type": attack_type,
        "poison_pct": poison_pct,
        "defense": defense,
        "test_accuracy": test_acc,
        "weighted_f1": weighted_f1,
        "macro_f1": macro_f1,
        "pos_precision": pos_precision,
        "pos_recall": pos_recall,
        "tn": tn,
        "fp": fp,
        "fn": fn,
        "tp": tp,
        "num_test_samples": num_test,
    }


def write_header(md_path):
    md_path.parent.mkdir(parents=True, exist_ok=True)
    with md_path.open("w") as f:
        f.write("# Evaluation Results Summary\n\n")
        f.write("_This table was generated by `scripts/generate_eval_table.py`._\n\n")
        # Empty table header
        f.write("| " + " | ".join(COLUMNS) + " |\n")
        f.write("|" + "---|" * len(COLUMNS) + "\n")


def append_row(md_path, row):
    with md_path.open("a") as f:
        values = [str(row.get(col, "")) if row.get(col, "") is not None else "" for col in COLUMNS]
        f.write("| " + " | ".join(values) + " |\n")


def main():
    json_files = sorted(EVAL_DIR.rglob("*.json"))
    write_header(OUT_MD)

    for p in json_files:
        try:
            d = json.loads(p.read_text())
        except Exception as e:
            print(f"Skipping {p}: failed to parse JSON: {e}")
            continue
        metrics = extract_metrics(d)
        row = {"model_folder": p.parent.name, "file": p.name}
        row.update(metrics)
        append_row(OUT_MD, row)


if __name__ == "__main__":
    main()
