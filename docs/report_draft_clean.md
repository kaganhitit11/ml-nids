# Introduction

In today's world, edge devices are interconnected over sophisticated networks all around the globe. These networks move trillions of packets every day with various intentions—such as web browsing, video and audio streaming, file transfers, remote administration, financial transactions, and sensor data collection—among many others. 

However, because any device connected to the World Wide Web can be addressed, anybody—especially intruders—can send malicious packets or execute harmful activities, for example jamming normal traffic by launching denial-of-service attacks. Such incidents can have serious consequences, ranging from service outages and data breaches to damage that can affect critical infrastructure and even national-level organizations. 

Thus, in order to preserve the safety of both communicating parties, packets must be investigated thoroughly before being accepted as valuable information bits. For this investigation, network intrusion detection systems (NIDS) have been created. NIDS inspect network traffic and attempt to classify behaviors into benign and malicious, providing protection against potential threats. However, the success of these systems is not trivial. Building an effective NIDS requires detailed experiments, wise design choices, and a deep understanding of everyday network activities carried out by security professionals. 

In practice, NIDS performance is often framed in terms of two types of errors. A false negative—where the system fails to detect an ongoing attack—can allow intruders to compromise systems, steal data, or disrupt services with little or no response. A false positive—where the system flags legitimate traffic as malicious—can overwhelm analysts with alerts, leading to alert fatigue and potentially causing real attacks to be missed in the noise. Balancing these errors is a central challenge in NIDS design and deployment. 


With the increased availability of large datasets and advances in machine learning techniques, efforts on NIDS have intensified. Researchers commonly evaluate their approaches on benchmark datasets such as KDD Cup 99 \cite{kdd99}, NSL-KDD \cite{nslkdd}, UNSW-NB15 \cite{unsw}, and CIC-IDS2017 \cite{cic}, which provide labeled examples of normal and malicious traffic. Machine learning models—including logistic regression, random forests, and neural networks—are trained on these labeled datasets to distinguish benign from malicious traffic. However, this reliance on labeled training data introduces a critical vulnerability: if an adversary can corrupt the labels during data collection or annotation, the resulting model may learn to misclassify traffic in ways that benefit the attacker.

This threat, known as *label poisoning* or *label flipping*, is a training-time attack where an adversary manipulates a fraction of the training labels without altering the underlying features. Unlike adversarial examples that perturb inputs at inference time, label poisoning corrupts the model before deployment. The attack can be *untargeted*—randomly flipping labels to degrade overall accuracy—or *targeted*, where specific attack classes are relabeled as benign to create blind spots in the detector. Targeted label poisoning is particularly dangerous because it allows adversaries to selectively suppress detection of high-value intrusions while maintaining acceptable overall accuracy, making the attack difficult to notice.

Despite the operational relevance of such structured label-poisoning threats, existing evaluations are often limited to single datasets or narrow model classes, leaving cross-dataset vulnerability poorly characterized. Prior work has demonstrated label poisoning in image classification and spam filtering, but systematic studies on NIDS across diverse network traffic distributions remain scarce.

In this work, we address this gap through a comprehensive cross-dataset study of targeted label-poisoning attacks and practical defenses for NIDS. We evaluate four complementary benchmark datasets—UNSW-NB15, CIC-IDS2017, CUPID, and CIDDS-001—selected to represent diverse network environments, attack distributions, and class imbalance characteristics. We employ a representative model suite spanning traditional machine learning (Logistic Regression, Random Forest) and neural architectures (MLP, CNN, and RNN) to understand how model complexity affects vulnerability. Under bounded label-flip budgets (5\%, 10\%, 20\%), we implement five structured poisoning strategies: class hiding, feature-targeted poisoning, influence-aware poisoning, disagreement-based poisoning, and temporal window poisoning (applied exclusively to CUPID due to its temporal structure). We further evaluate removal and reweighting defenses to characterize their effectiveness across attack intensities.

Our experiments reveal alarming vulnerabilities. On CIC-IDS2017, a class hiding attack with only 10\% label poisoning reduced attack recall from 98\% to 16\%, and at 20\% poisoning, the model achieved 0\% attack detection—complete failure to identify any malicious traffic. Critically, overall accuracy remained above 80\% in these scenarios, creating an *accuracy illusion* where the model appears functional but is effectively blind to attacks. Defense mechanisms showed limited effectiveness: removal defense provided some protection at low poisoning rates but collapsed at higher rates, while reweighting defenses were largely ineffective across all scenarios. We also observed significant dataset-specific vulnerability patterns, with CIC-IDS2017 being most susceptible and UNSW-NB15 showing greater resilience.

The contributions of this work are:
- **Systematic cross-dataset evaluation**: We provide the first comprehensive study of label poisoning across four diverse NIDS benchmark datasets with consistent experimental methodology.
- **Attack strategy comparison**: We implement and compare five structured poisoning strategies, identifying class hiding as the most effective attack and characterizing the conditions under which each strategy succeeds.
- **Defense analysis**: We evaluate practical training-time defenses (removal and reweighting) and document their failure modes at higher poisoning rates.
- **The accuracy illusion**: We identify and characterize the dangerous phenomenon where poisoned models maintain high overall accuracy while completely failing to detect attacks—a critical finding for practitioners who rely on accuracy as a health metric.


## Related Work

This section reviews the literature that motivated and informed our study, organized into four thematic areas: (1) the evolution of network intrusion detection systems and benchmark datasets, (2) foundational research on data poisoning attacks, (3) label-flipping as a specific and effective attack vector, and (4) defense mechanisms proposed to mitigate such threats. By tracing the development of these research threads, we identify the gaps that our cross-dataset evaluation aims to address.

### 2.1 Network Intrusion Detection Systems and Benchmark Datasets

The need for automated network security monitoring emerged alongside the rapid expansion of the Internet in the 1990s. One of the earliest and most influential systems was **Bro** (now known as Zeek), developed by Paxson in 1999 \cite{paxson1999}. Bro introduced a novel architecture that separated the low-level event engine from a high-level policy script interpreter, enabling flexible, protocol-aware analysis of network traffic in real time. This design philosophy—combining deep packet inspection with scriptable detection logic—established a template that many subsequent systems would follow.

As machine learning techniques matured, researchers began applying them to intrusion detection, hoping to automate the discovery of attack patterns and reduce reliance on hand-crafted signatures. However, Sommer and Paxson (2010) \cite{sommer2010outside} offered a critical assessment of this trend in their influential paper "Outside the Closed World." They argued that many ML-based NIDS evaluations suffered from unrealistic assumptions: closed-world datasets with artificial attack-to-benign ratios, lack of concept drift, and evaluation metrics that failed to capture operational costs. Their work called for more rigorous, realistic evaluation practices—a challenge that remains relevant today and directly motivates our cross-dataset methodology.

The quality and representativeness of benchmark datasets has been a persistent concern. The **KDD Cup 1999** dataset \cite{kdd99}, derived from the 1998 DARPA intrusion detection evaluation, became the de facto standard for over a decade despite well-documented limitations including redundant records, unrealistic traffic distributions, and outdated attack patterns. Tavallaee et al. (2009) \cite{tavallaee2009} addressed some of these issues by creating the **NSL-KDD** dataset, which removed duplicate records and rebalanced the data, though fundamental concerns about age and realism remained.

More recent efforts have produced datasets that better reflect contemporary network environments. Moustafa and Slay (2015) \cite{moustafa2015unsw} introduced **UNSW-NB15**, generated in a controlled cyber-range environment with nine modern attack categories and 49 features extracted using Argus and Bro-IDS tools. Sharafaldin, Lashkari, and Ghorbani (2018) \cite{sharafaldin2018} created **CIC-IDS2017** at the Canadian Institute for Cybersecurity, featuring realistic benign background traffic and diverse attack scenarios including brute-force, DoS, web attacks, and infiltration. Lawrence et al. (2022) \cite{lawrence2022cupid} developed **CUPID**, a dataset capturing professional penetration testing traffic with explicit temporal structure, enabling evaluation of time-aware detection methods. Ring et al. (2017) \cite{cidds} contributed **CIDDS-001**, providing enterprise-scale NetFlow data with labeled attack traffic.

Recent survey work by Khraisat et al. (2019) \cite{khraisat2019} and Chou and Jiang (2021) \cite{chou2021} has synthesized this landscape, cataloging available datasets, comparing detection techniques, and identifying open challenges. Goldschmidt and Chudá (2025) \cite{goldschmidt2025survey} further systematized dataset limitations, providing recommendations that guided our selection of complementary benchmarks. A key insight from this literature is that conclusions drawn from a single dataset rarely generalize—motivating our decision to evaluate across four diverse benchmarks (UNSW-NB15, CIC-IDS2017, CUPID, and CIDDS-001) with different traffic characteristics, attack distributions, and class imbalance profiles.

### 2.2 Data Poisoning Attacks

While considerable effort has focused on building accurate intrusion detection models, less attention has been paid to their robustness against adversarial manipulation of training data. **Data poisoning** refers to attacks where an adversary corrupts the training set to degrade model performance or induce targeted misclassifications at inference time.

The seminal work by **Biggio, Nelson, and Laskov (2012)** \cite{biggio2012} established the theoretical and algorithmic foundations for poisoning attacks against Support Vector Machines. They demonstrated that an adversary could craft malicious training points using gradient ascent on the validation error surface, significantly increasing the classifier's test error. Crucially, their attack assumed only that the adversary could inject data into the training pipeline—a realistic threat model for systems that collect training data from untrusted sources such as honeypots or user-reported samples. This work showed that the standard machine learning assumption of well-behaved, i.i.d. training data does not hold in security-sensitive settings.

Building on this foundation, **Steinhardt, Koh, and Liang (2017)** \cite{steinhardt2017certified} developed a theoretical framework for certified defenses against data poisoning. Their key contribution was constructing upper bounds on the worst-case loss a defender can suffer against any poisoning attack within a specified budget. They showed that defense effectiveness is highly dataset-dependent: while MNIST and Dogfish image datasets proved resilient (achieving at most 4% error even with 30% poisoned data under an oracle defense), the IMDB sentiment corpus was driven from 12% to 23% error with only 3% poisoned data. This finding—that high-dimensional datasets with many irrelevant features are more vulnerable—has important implications for network intrusion detection, where feature spaces are often large and heterogeneous.

Several comprehensive surveys have synthesized the growing literature on poisoning attacks. **Wang et al. (2022)** \cite{wang2023threats} provide an extensive taxonomy covering both traditional machine learning and deep learning systems. They categorize attacks by adversarial goals (untargeted vs. targeted), knowledge assumptions (white-box, gray-box, black-box), and attack mechanisms (label poisoning, feature poisoning, model poisoning). Their presentation of the bilevel optimization framework—where the outer optimization crafts poisoned samples to maximize validation loss while the inner optimization trains the victim model—provides a unified lens for understanding diverse attack strategies. More recently, **Zhao et al. (2025)** \cite{zhao2025} extended this analysis specifically to deep learning, discussing emerging threats to large language models and identifying open challenges in defense design.

### 2.3 Label-Flipping Attacks

Among poisoning strategies, **label-flipping** (also called label poisoning) stands out for its simplicity and effectiveness. In this attack, the adversary corrupts only the labels of training samples while leaving the features unchanged. Despite its simplicity, label-flipping can cause substantial damage, particularly when applied in a targeted manner—flipping labels of a specific class to create blind spots in the classifier.

**Jebreel et al. (2022)** \cite{jebreel2022} studied label-flipping in the context of federated learning, where malicious participants can poison their local data before contributing model updates. They demonstrated that label-flipping is both easy to perform and difficult to detect, significantly degrading accuracy on the source class (the class whose labels are flipped). Their analysis revealed that the attack's impact scales with both the proportion of malicious participants and the number of flipped samples. While their proposed defense targets federated settings specifically, their characterization of label-flipping's effectiveness applies broadly.

Chang, Dobbie, and Wicker (2023) \cite{chang2023fastflip} developed efficient algorithms for label-flipping attacks on tabular data, demonstrating that such attacks remain potent even when the adversary operates under tight computational or query budgets. Their work is particularly relevant to NIDS, which typically process tabular flow-level features.

A critical insight from this literature is that label-flipping attacks can be **targeted**—selectively flipping labels of attack traffic to benign, thereby suppressing detection of specific intrusion types while maintaining acceptable overall accuracy. This "accuracy illusion" makes the attack particularly dangerous in operational settings where defenders may monitor aggregate metrics without examining per-class performance.

### 2.4 Defense Mechanisms

Defending against data poisoning requires identifying and mitigating the influence of corrupted samples before or during training. The literature has explored several approaches:

**Outlier removal** is perhaps the most intuitive defense: identify samples that deviate significantly from the expected data distribution and exclude them from training. Steinhardt et al. (2017) \cite{steinhardt2017certified} analyzed defenses that remove outliers outside a feasible set defined by class centroids. While effective against some attacks, they showed that defenses relying on empirical (potentially poisoned) centroids can be subverted when the attacker controls a larger fraction of the data.

**Loss-based filtering** identifies suspicious samples by their training loss: samples that consistently incur high loss may be mislabeled or adversarially crafted. This approach is computationally inexpensive and can be applied to any differentiable model. However, it assumes that poisoned samples behave differently from clean samples in terms of loss—an assumption that sophisticated attacks may violate.

**Reweighting** offers a softer alternative to removal: rather than discarding suspicious samples entirely, reduce their contribution to the training objective. This preserves potentially useful information while limiting the influence of outliers. Koh and Liang (2017) \cite{koh2017influence} developed influence functions that estimate each training sample's effect on test predictions, providing a principled basis for identifying and down-weighting harmful samples.

**Ensemble disagreement** exploits the observation that poisoned samples often induce inconsistent predictions across independently trained models. By training multiple models with different initializations or architectures and flagging samples where predictions disagree, defenders can identify ambiguous or adversarially manipulated regions of the feature space. This strategy draws on the query-by-committee framework introduced by Seung, Opper, and Sompolinsky (1992) \cite{seung1992qbc}.

### 2.5 Gaps in Prior Work and Our Contributions

Despite the substantial body of work reviewed above, several gaps remain:

1. **Limited cross-dataset evaluation.** Most poisoning studies evaluate on a single dataset, making it difficult to assess whether findings generalize across different traffic distributions, feature spaces, and class imbalance profiles. Sommer and Paxson's critique of closed-world evaluation applies equally to adversarial robustness studies.

2. **Narrow model coverage.** Many studies focus on a single model family (e.g., SVMs, neural networks) without systematically comparing how poisoning affects models with different inductive biases—from linear classifiers to ensemble methods to deep networks.

3. **Disconnect between poisoning research and NIDS.** While poisoning attacks have been extensively studied in image classification and spam filtering, systematic evaluations on network intrusion detection benchmarks remain scarce. The unique characteristics of NIDS data—high dimensionality, severe class imbalance, temporal structure, and heterogeneous attack types—warrant dedicated investigation.

4. **Defense effectiveness under varying attack intensities.** Prior work often evaluates defenses at fixed poisoning rates without characterizing how effectiveness degrades as attack intensity increases.

Our work addresses these gaps through a **systematic cross-dataset study** of label-poisoning attacks on ML-based NIDS. By evaluating four benchmark datasets (UNSW-NB15, CIC-IDS2017, CUPID, CIDDS-001), five model architectures (Logistic Regression, Random Forest, MLP, CNN, RNN), five poisoning strategies (class hiding, feature-targeted, influence-aware, disagreement-based, temporal window), and two defense mechanisms (removal, reweighting) across multiple poisoning rates (5%, 10%, 20%), we provide comprehensive empirical evidence on the vulnerability of NIDS to training-time attacks and the conditions under which defenses succeed or fail.
